\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{scribe}
\usepackage{listings}
\usepackage{fullpage}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{parskip}
\usepackage[svgnames]{xcolor}
\usepackage{color}
% \definecolor{light-gray}{gray}{0.90}
% \lstset{backgroundcolor=\color{light-gray},showlines=true}

% \usepackage{xcolor}
% \usepackage{listings}

% \lstdefinestyle{BashInputStyle}{
%   language=bash,
%   basicstyle=\small\sffamily,
%   numbers=left,
%   numberstyle=\tiny,
%   numbersep=3pt,
%   frame=tb,
%   columns=fullflexible,
%   backgroundcolor=\color{yellow!20},
%   linewidth=0.9\linewidth,
%   xleftmargin=0.1\linewidth
% }


\usepackage{minted}
\setminted{fontsize=\footnotesize,baselinestretch=0.5}

\Scribe{}
\Lecturer{Queenie Qiu, John Raiti. Student: \textbf{Shucheng Guo, Harry Tung, Joshua Sanchez}}
\LectureNumber{3}
\LectureDate{DATE: Jan 26th. 2023}
\LectureTitle{Physical Implementations and Introduction to Control}

\lstset{style=mystyle}

\begin{document}
	\MakeScribeTop

\setlength{\parindent}{15pt}

%#############################################################
%#############################################################
%#############################################################
%#############################################################


\section {Sensor inaccuracies in the physical robot}
\subsection{Connecting to the physical robot}

Connecting to the physical robot according to the instructions in the slides. 

\subsection{Errors in odometry in real world operation}

In this section, we will use the patrol example that we have explored in the Lab1 again so as to compare the odometry performance between simulation and real world implementation. We will use the nodes and code created in Lab1 to visualize data from the /odom topic.

\textbf{Deliverables:}
\begin{enumerate}
    
    \item Run the patrol example with the following parameters: square length 0.8m and 1 iteration. In an additional terminal run the plot\_odom.py file.
    
    \begin{minted}{bash}
      $ roslaunch turtlebot3_example turtlebot3_client.launch
      
      $ rosrun turtlebot3_example turtlebot3_server
    \end{minted}

    \begin{enumerate}

        \item Make notes about robot behaviors: do the wheels turn at the same rate when moving forward? Were the 90 degree turns accurate? Does the robot return to the starting position?
        
        \textbf{Answer: }The wheels would rotate at the same rate when moving forward without the impact of other factors, e.g. low battery, facing obstacles.
        \\The turns, however, were far from accurate. The robot wouldn't make perpendicular turns in square patrols, not to mention gradual ones in circular turns.
        \\As expected, the robot would never return to its original position with only the joint states from odometry in real world.

        \item Plot the x-y position of the robot against the “ground truth” and take a screenshot.
        
        \begin{figure}[H]
          \centering\includegraphics[width=14cm]{images/gmapping.png}
          \caption{Plotting a square robotic patrol with the command \mintinline{bash}{s 0.8 1}.}
          \label{fig:gmapping}\vspace{-10pt}
          \end{figure}

        \item Compare the graph to the one obtained in Lab1. Mention differences. Are odom errors larger or smaller? Mention what can be influencing the results.
        
        \textbf{Answer: }Compared to the graph from Lab1, which is based on simulation, this real-world odometry-supported robot obviously performed much worse. The underperforming areas include the errors in its turns, differences between start and stop points, and the ability in walking at an even pace.
        \\It's assumed that the context made a huge difference. When simulated, the environment, in which the route is planned, is nearly ideal. But in the real world, more things factor in when the odometry is sensoring information. It needs more input from more types of sensors to jointly plan for the route.
    
    \end{enumerate}

    \item What can be done to improve the accuracy of sensor data or to get better estimation of robots' positions and orientations?
    
    \textbf{Answer: }We can calibrate the sensors against ground truth, combine passive sensors and active ones, and prime the robot with a static map.

\end{enumerate}


\subsection{Errors in laser readings in real world operation}
In this section, we will go through the mapping procedure followed in Lab2, and store the /scan topic data into a rosbag. We will compare the gmapping results in the real world against the ones obtained in the simulation.

\begin{enumerate}

    \item Make sure the physical turtlebot is operational and placed correctly, in the environment you want to map. Move the robot to the right bottom corner of the map.
    
    \begin{figure}[H]
    \centering\includegraphics[width=14cm]{images/map.jpeg}
    \caption{Start Position Indication}
    \label{fig:pid_1}\vspace{-10pt}
    \end{figure}
    
    As a reminder the command line to launch the mapping is as follows:
    \begin{minted}{bash}
    $ roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping
    \end{minted}
    Note: you will need to run the teleoperation program to go through the map.
    \begin{minted}{bash}
    $ roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
    \end{minted}
    \item Additionally, before you start teleoperating the robot, record a rosbag file with the following topics.
    \begin{minted}{bash}
    $ rosbag record /odom /scan /cmd_vel /tf -O physicaltb3_map.bag
    \end{minted}
    
    \item When you have finished tracing the surroundings to complete the map, you can stop the bag file recording and save the resulting map by running the following command in the terminal before finishing the gmapping launch.
    
    \begin{minted}{bash}
        $ rosrun map_server map_saver -f ~/gix_map
    \end{minted}
    
    
\end{enumerate}

\textbf{Deliverables:}
\begin{enumerate}
    \item Inspect the resulting files of the maps: the one obtained in the simulation and the one from the physical implementation. Attach screenshots of both pgm files. Mention differences between the results obtained in the simulation and the real world in terms of: thickness of the edges, additional shapes outside of the intended map area, differences in parameter values found in the .yaml file.
    
    \begin{figure}
        \centering
        \subfloat[Digitally simulated map of the hexagon area.]{\includegraphics[width=.45\linewidth]{images/hex_map.png}}
        \hfill
        \subfloat[Physically implemented map of the GIX setup.]{\includegraphics[width=.45\linewidth]{images/gix_map.png}}
        \caption{Comparing two maps achieved in different contexts.}
        \label{fig:map_pgm}\vspace{-10pt}
    \end{figure}

    \textbf{Answer: }Between the two maps sensored and generated by the robot, there are similarities and differences. The edges detected in the physical map of GIX are significantly thicker than the ones in the hexagon. There are more extraneous shapes detected and marked in the simulated map, while the physical implementation is more clear-cut. This could be personal, since not everyone has the messy simulation like mine. The parameter values, especially the occupancy and free thresholds, are identical in both .yaml files.
    
    \item Use that map you just created and launch the navigation example in Lab2. Return the robot to the start position on the physical map. You may want to use that spot to give a 2DPoseEstimate as your starting position.
    \begin{minted}{bash}
        $ roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file
        :=$HOME/gix_map.yaml
    \end{minted}
    \begin{enumerate}

        \item Set a 2DNavGoal on the map that is close to this area.
        
        \begin{figure}[H]
        \centering\includegraphics[width=14cm]{images/map2.jpeg}
        \caption{2DNavGoal Position Indication}\vspace{-10pt}
        \end{figure}
        
        What happens when a 2D navigation goal is provided? How long does it take for your robot to plan and reach the goal?

        \textbf{Answer: }After a goal is set, the robot takes some time to plan its path, adjusts its direction, walks the route, makes turns and avoids the obstacles along the way.
        \\If it bumps into obstacles, both in map and in reality, the path will be replanned for a couple of times before an official failure due to possible collisions.
        \\For our robot, it took less than one minute to arrive at the goal on average. The time could be, however, significantly longer if the destination is chosen poorly.

        \item How close did the robot get to the goal (distance from the goal)?
        
        \textbf{Answer: }The robot was very close to the goal, with an error of several inches, if there was any.

        \item Discuss your observations. These should include quantitative and qualitative components. You may rely on comparisons based on the concentric rings shown around the goal, or you may reference your comparisons to the robot’s interpretation of the world.
    
        \textbf{Answer: }Based on my observations of 10 robotic patrols, I found the alignment between maps essential. Since there is an error in the starting position, the robot has to be manually put in a position away from the cross that aligns both maps, or it will fail. The failure rates accounted for roughly 50\% of the attempts, for reasons such as misaligned maps, dynamic obstacles and tough destinations. To avoid such failures, it's recommended to place the robot and select a target relatively in the middle of the lane to save enough space for route planning.
        \\The time for the robot to reach the target also varied a lot, from as fast as 45s to as slow as 2min. When it detected obstacles at the start or end points, it would replan the route over and over before it could work, or just not move at all. Around 20\% of the times, when the destination was poorly selected, the program would report too many errors until it declared a failure of potential collisions.

    \end{enumerate}

    \item Submit your recorded rosbag with your lab assignment.
    
    \textbf{Answer: }The recorded bag file is attached in the assignment.
    
\end{enumerate}


\section{Introduction to control strategies in robot navigation
}
In this section, we will explore different paradigms to control the movements of a robotic platform. In this case, we will focus on the changes in position and orientation of a wheeled robot. For convenience, we will return to the simulation setting.
This means that we will set our environment variables to its previous values of localhost in the .bashrc file. 

\subsection{Executing robot motions in open-loop control}

Open-loop refers to the robot’s operation created by the input signal does not depend on the system’s output. Specifically, we will create commands for the robot to move forward 1.5 meters.
\begin{enumerate}
    \item Launch a simulated turtlebot3 in the stage\_1 world.
    \begin{minted}{bash}
        $ roslaunch turtlebot3_gazebo turtlebot3_stage_1.launch
    \end{minted}
    \item By knowing the distance to be traveled, we can determine the constant speed and the amount of time that constant speed needs to be held to cover said distance. Use the command-tool rostopic to publish velocity commands to the robot:
    \begin{minted}{bash}
        $ rostopic pub -1 /cmd_vel geometry_msgs/
        Twist  '{linear:  {x: <value>, y: 0.0, z: 0.0}, angular
        : {x: 0.0, y: 0.0, z: 0.0}}'
    \end{minted}
    After a certain amount of time (pos = vel * time), you will publish a twist message to effectively stop the robot from moving:

    \begin{minted}{bash}
        $ rostopic pub -1 /cmd_vel geometry_msgs/Twist '{linear
        :  {x: 0.0, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}'
    \end{minted}
    
    For your convenience, we have prepared a simple python script that submits the commands to the terminal by using the os library. You will need to complete your selected values for time and speed.
    
    
    
\end{enumerate}

\textbf{Deliverables:}
\begin{enumerate}

    \item Report the chosen strategy (published values) to complete the task of moving 1.5m forward in open-loop.
    
    \textbf{Answer: }For the command to publish velocity, I chose to apply \mintinline{bash}{0.15} to the linear velocity of \mintinline{bash}{x}, and stopped right after the topic published for $3$ seconds, which made the expected distance to travel $1.5m$ ($0.15 \times 10.0$).
    
    \item Inspect the turtlebot3’s position in Gazebo. (World -> Models -> turtlebot3\_burger -> pose -> x). Did the robot move 1.5m accurately?
    
    \textbf{Answer: }It didn't move exactly $1.5m$ forward, but a bit shorter than that, around $1.48m$, and slightly to the left or right. Depending on the speed or linear velocity, the error between expected and realistic distances could be smaller or greater.
    
    \item Compare your desired traveled value of 1.5m with the actual position of the robot in gazebo, and the reported value in the /odom topic (use rostopic echo /odom). Discuss differences.
    
    \textbf{Answer: }As mentioned in the last question, the actual travel of the robot is slightly longer than expected and tilted to the left or right.
    \\The pose information as reported in \mintinline{bash}{Gazebo} and actual location as published in \mintinline{bash}{/odom} mostly agree with each other. However, since the messages are published more precisely and more frequently in \mintinline{bash}{/odom}, the pose information remains dynamic and decreases with time, though very slightly.
  
    \item Mention the challenges of operating the robot in open-loop, particularly when the motions increase in complexity.
    
    \textbf{Answer: }Compared to the closed-loop control system, open-loop lacks accuracy and versatility. Due to a lack of feedback structure, they produce inaccurate outputs, and they can't correct the output automatically. As a result, they cannot adapt to the variations in environmental conditions or external disturbances.
    \\Open-loop can be used in simple applications, since they are low in cost and easy to implement. But with the increase of motion complexity, there is a need for human monitoring for better outputs.

    \item How would the sequence of commands look if you wanted to complete a patrol pattern (e.g. a triangle) in open-loop? Report the solution in pseudocode, commands should include GoForward(distance),WaitTime(time), Rotate(angle in degrees).
    
    \vspace{-3ex}
    \begin{verbatim}
        Rotate(30)
        GoForward(0.5)
        WaitTime(3s)
        Rotate(120)
        GoForward(0.5)
        WaitTime(3s)
        Rotate(60)
        GoForward(0.5)
        WaitTime(3s)
    \end{verbatim}
    \vspace{-4ex}

    \textbf{Answer: }Above is the pseudocode for a patrol of a equilateral triangle in the first quadrant with the bottom side sitting on the x-axis. The argument passed to the \mintinline{bash}{GoForward} function is the velocity, times \mintinline{bash}{WaitTime} to get the distance to walk. The argument passed to \mintinline{bash}{Rotate} dictates the degrees to turn right.

    \item Modify the given open loop file to achieve this patrol motion.
    
    \textbf{Answer: }The modified python script is attached in the submission, and agrees with the above pseudocode.

    \item How close to the start location did your robot finish? Use values from both: odometry and gazebo pose.
    
    \begin{table}[H]
    \centering
    \begin{tabular}{|c|cc|cc|}
    \hline
    \multirow{2}{*}{Pose} & \multicolumn{2}{c|}{Attempt 1}                  & \multicolumn{2}{c|}{Attempt 2}                  \\ \cline{2-5} 
                                   & \multicolumn{1}{c|}{\textit{Gazebo}} & \textit{Odometry} & \multicolumn{1}{c|}{\textit{Gazebo}} & \textit{Odometry} \\ \hline
    \textit{x} & \multicolumn{1}{c|}{1.089103}  & 1.0875463  & \multicolumn{1}{c|}{-1.592979} & -1.5919701 \\ \hline
    \textit{y} & \multicolumn{1}{c|}{-1.068893} & -1.0695119 & \multicolumn{1}{c|}{-0.358039} & -0.3571131 \\ \hline
    \textit{z} & \multicolumn{1}{c|}{-0.001002} & -0.0010022 & \multicolumn{1}{c|}{-0.001002} & -0.0010016 \\ \hline
    \end{tabular}
    \end{table}
    \vspace{-2ex}
    
    \textbf{Answer: }In all of my attempts, the robots ended up very far from the designated target. The ending poses didn't follow a pattern that could be induced, but rather were randomly scattered in the whole space. In the table, two of the representative attempt were chosen and recorded.

\end{enumerate}

\subsection{Executing robot motions in closed-loop control}
In closed-loop operation, the robot’s input signal depends on a reference value that we want the output to be, and the comparison against the robot’s current output through sensor feedback. We will create commands for the robot to move forward 1.5 meters by using the readings of some of its sensors, namely odometry and laser.

\begin{enumerate}
    \item Create a node that subscribes to the /odom topic and publishes to the /cmd\_vel topic. It will read the initial /odom value and only stop publishing commands to the velocity topic when the current value increases by 1.5m. Use the file close\_loop\_odom.py as a starting point.
    
    \item Create a node that subscribes to the /scan topic and publishes to the /cmd\_vel topic. It will read the initial /scan value corresponding to the front of the robot and only stop publishing commands to the velocity topic when the current scan value decreases by 1.5m. Use the file close\_loop\_laser.py as a starting point.

\end{enumerate}

\textbf{Deliverables:}
Run your closed-loop nodes to move the robot 1.5 meters forward and compare the performance of using /odom vs. /scan. Also check the robot’s position in Gazebo through the plotting utility and compare to answer following questions:
\begin{enumerate}
    \item How do each closed-loop control compare to the open-loop performance?
    
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|cl|cl|cl|}
        \hline
        Pose       & \multicolumn{2}{c|}{\textit{Open: Time + Vel}} & \multicolumn{2}{c|}{\textit{Closed: Odometry}} & \multicolumn{2}{c|}{\textit{Closed: Laser Scan}} \\ \hline
        \textit{x} & \multicolumn{1}{c|}{1.465127}    & 1.472632    & \multicolumn{1}{c|}{1.559412}    & 1.588417    & \multicolumn{1}{c|}{1.528828}     & 1.556617     \\ \hline
        \textit{y} & \multicolumn{1}{c|}{-0.013769}    & 0.075753    & \multicolumn{1}{c|}{0.001973}    & 0.007014    & \multicolumn{1}{c|}{0.005801}     & 0.005815     \\ \hline
        \textit{z} & \multicolumn{1}{c|}{-0.001002}   & -0.001002   & \multicolumn{1}{c|}{-0.001002}   & -0.001002   & \multicolumn{1}{c|}{-0.001002}    & -0.001002    \\ \hline
        \end{tabular}
        \end{table}
        \vspace{-2ex}

    \textbf{Answer: }Compared to the open-loop robot patrol, the closed-loop ones utilizing sensored data were slightly better in performance, but quite inconsistent. Based on the limited data, controlling with laser scanned data was more accurate in path planning, but controlling with merely time and velocity wasn't good for nothing. It remains unknown why sometimes performances of closed-loops would be worse, and it calls for us to conduct more experiments on whether obstacles or robot types makes a difference.
    \\From the table, it can be seen that using sensors to move $1.5m$ forward, whether \mintinline{bash}{/odom} or \mintinline{bash}{/scan}, yields better results in maintaining the original angle -- walking in straight line. Determining only time and velocity for the robot, without the use of sensors, in fact controls better the distance to walk. It's worth mentioning choosing different linear velocities lead to distinct results, since the faster speed, the more inertia.

    \item Record a video of the robot’s performance for each closed-loop control node. Attach your video links.
    
    \href{https://drive.google.com/file/d/1X76EqsRq0fVXSCenQvJV1aQPNA97RqPi/view?usp=sharing}{Google Drive link to the robotics motions by closed-loop and open-loop controls}

\end{enumerate}

\section{Extra credit: Simulated Wall-following behavior}
Now that we have completed motion using open and closed-loop control, we will go further into closed-loop by adding a controller step. Instead of using the error between the difference of an output measurement of our system and a specified reference value, the controller will use this error as input and output values to be used directly by the system. In our particular case of the simulated turtlebot3, we will use the readings from the laser sensor and a reference value of 0.5m to keep the robot driving parallel to a wall in the environment. 
We will provide code that you will need to complete, where a controller will use the error in the distance to the wall to output values from a PID controller that will modify the /cmd\_vel values

\begin{enumerate}
    \item Download the wall\_follower and PID python files from the course materials and add them to your workspace (e.g. turtlebot3/turtlebot3\_example/nodes folder). You will need to use the chmod +x <filename> command to make both files executables.
    \begin{minted}{bash}
        $ rosrun turtlebot3_example wall_follower.py
    \end{minted}
    
    \item Complete the code by selecting the correct topics to subscribe and publish, making sure that the readings from the lidar sensor are used correctly (you are interested in keeping a wall to the right side of the robot at a 0.5 distance at all times, this maps to a specific position in the ranges array).
    
    \item Launch a simulated turtlebot3 burger in the empty world in Gazebo. Change the initial position of the turtlebot3.
    \begin{minted}{bash}
        $ roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch x_pos
        :=-19.0 y_pos:=0.0
    \end{minted}
    
    \item Go to the Insert model tab in Gazebo’s left panel and add a “Grey Wall”. Go into the Model Editor to modify the geometry of the link in both visual and collisions tab (X = 20.0). Set the pose of the new long grey wall to x=-0.8, y=0.0
    
    \item Run (rosrun) the wall\_follower node. We will change the values in the main function of the file to modify the PID controller parameters and observe performance. Every time you terminate the launch you need to publish a /cmd\_vel message with zeros to stop the turtlebot3. You also need to reset the world poses in Gazebo (Ctrl+Shift+R). This step may have to be done several times before you see the robot in its original pose.
    
    
\end{enumerate}

\textbf{Deliverables:}

\begin{enumerate}
    \item Run the plotting node (plot\_odom.py) to visualize the change of the error value over time with the different PID parameters values and save the plots for each case. You can also use the Gazebo plotting Utility and export the resulting plot.
    
    \begin{figure}[b]
    \centering
    
    \subfloat[No PID: \mintinline{bash}{P=1, I=0, D=0}]{\includegraphics[width=0.35\columnwidth]{images/1_0_0.png}}
    \subfloat[Only P: \mintinline{bash}{P=1.2, I=0, D=0}]{\includegraphics[width=0.35\columnwidth]{images/1.2_0_0.png}}
    \subfloat[PI: \mintinline{bash}{P=1.2, I=0.1, D=0}]{\includegraphics[width=0.35\columnwidth]{images/1.2_0.1_0.png}}\\

    \subfloat[PD: \mintinline{bash}{P=1.1, I=0, D=1.5}]{\includegraphics[width=0.35\columnwidth]{images/1.1_0_1.5.png}}
    \subfloat[PID: \mintinline{bash}{P=1.2, I=0.25, D=1.5}]{\includegraphics[width=0.35\columnwidth]{images/1.2_0.25_1.5.png}}
    \subfloat[PI:\mintinline{bash}{P=1.2, I=0.25, D=0}]{\includegraphics[width=0.35\columnwidth]{images/1.2_0.25_0.png}}\\

    \subfloat[ID: \mintinline{bash}{P=1, I=0.1, D=10}]{\includegraphics[width=0.35\columnwidth]{images/1_0.1_10.png}}
    \subfloat[I: \mintinline{bash}{P=1, I=0.1, D=0}]{\includegraphics[width=0.35\columnwidth]{images/1_0.1_0.png}}
    \subfloat[D:\mintinline{bash}{P=1, I=0, D=10}]{\includegraphics[width=0.35\columnwidth]{images/1_0_10.png}}\\

    \subfloat[Neg P: \mintinline{bash}{P=0.8, I=0, D=0}]{\includegraphics[width=0.35\columnwidth]{images/0.8_0_0.png}}
    \subfloat[PID: \mintinline{bash}{P=0.8, I=0.1, D=10}]{\includegraphics[width=0.35\columnwidth]{images/0.8_0.1_10.png}}
    \subfloat[Inf D: \mintinline{bash}{P=1, I=0, D=100}]{\includegraphics[width=0.35\columnwidth]{images/1_0_100.png}}

    \caption{Plotting robot positional changes with different sets of PID parameters.}
    \label{fig:PID}
    \end{figure}

    \textbf{Answer: }Refer to the 12 subplots in Figure \ref{fig:PID} of the robotic responses to various PID parameter settings. The x-axis represents the robot's x coordinate, or horizontal position in the 2D space. The y-axis represents the difference between the expected and actual distances between its right side and the wall.
    
    \textsc{Notice: }The pose settings of the models in the Gazebo world are changed for successful execution of the program and analysis of the plots. Specifically, the original position of the robot is \mintinline{bash}{(-10, 0, 0)}, while the wall is \mintinline{bash}{(0, -0.5, 0)},
    with a default difference of $0.5m$, exactly the distance to keep.

    \item Describe what is happening in each of the 6 plots you create. For example: time for error to go to zero, is it oscillating, is there stationary error?
    
    \begin{figure}[H]
        \centering\includegraphics[width=14cm]{images/pidparameter.png}\vspace{-10pt}
        \caption{Parameters for PID.}
        \label{fig:pid_2}
        \vspace{-2ex}
    \end{figure}
    
    For the custom values of the PID, you should aim to have a response that goes fast to the desired reference, has little or no oscillation in the response and its response is centered on the desired response (no stationary error)

    \textbf{Answer: }For the required sets of PID values, with my pose settings, the robot is consistently unstable as the oscillations become larger with time. Despite no stationary errors, the existing errors don't go away, creating growing oscillations, and the robot may even be out of control before the path ends. This applies to plots a through f, along with h and j. Alternatively speaking, tuning only P and I doesn't make a difference.
    
    After raising D to $10$, with different PI settings, the robot gets more stable, and it takes only 10 seconds to clear the errors. As can be seen in g, i and k, tuning P and I with a constant D doesn't make a noticeable difference.

    Based on the observations, increasing D significantly is expected to boost the performance and bring down the time it takes to rule out the errors. On the contrary, it creates a stationary error of around $1$, after some instability like drifting.

    In conclusion, for this specific case, the derivative term has more control over the behavior of the robot. Increasing it to a reasonable range dampens the oscillations and eliminates the errors, but a stationary error will be introduced beyond that range.
    
    \item Compare the performance of the six controllers mentioned in the previous table in terms of:
    a. Time it takes to reach the desired goal (error=0).
    b. Peak oscillation value.
    c. Stationary error (value at which the robot settles and moves forward).

    \textbf{Answer: }In general, the three parameters in PID (Proportional, Integral, Derivative) impacts on the performance of the robot in walking in a straight line. Take the oscillation:
    P is related to its amplitude, I to its frequency and D to its damping.
    
    \begin{enumerate}
        \item For subplots other than g, i and k where D equals \mintinline{bash}{10}, the oscillations get larger, or unstable, which means they never reach the desired goal of zero error.
        \item The absolute values of the peaks in the oscillations are the errors, which mostly range from \mintinline{bash}{0.3} to \mintinline{bash}{0.5}. However, in better performances, the errors are kept close to zero.
        \item For stable controllers, 
    \end{enumerate}

    \item Describe some of the challenges you faced with this wall following task and the steps you took to overcome them.
    
    \textbf{Answer: }A couple of challenges came up in the experiments, the root causes of which can be categorized and analyzed as follows:

    \begin{itemize}
        \item Configuring the Gazebo models. To be honest, the difficulties that took most of the time were actually all related to the more insignificant issues. For example, it took me two days to find out if the robot was walking forward or backward; it took me even more time to correct the original pose settings of both models, which didn't work in the beginning. Some of these issues could be resolved by consulting others who are more familiar with using Gazebo, while some others by reading some online documentations and discussions, or experimenting with diverse options.
        \item Implementation of the wall following algorithm. Understanding how the algorithm works was not easy either, especially when the idea isn't taught in class. One of the examples is finding the correct indices of \mintinline{bash}{msg.ranges} at different angles, including \mintinline{bash}{right} and \mintinline{bash}{right_ahead_theta}, for distance calculation. Part of the reason for the failure is not knowing the robot's default orientation, leading to completely opposite results; another part is figuring out what the theta angle refers to here, and its difference with other wall follower solutions. The ways I tackle this issue include printing debug messages and testing different options, and comparing the codes with other algorithms.
        \item Tuning of the PID controller. Since the auto-tuning technique isn't available for this assignment, it has to be done by hand on a trial and error basis. At first, with the required sets of parameters, my robot is consistently unstable, as can be seen in the oscillations getting larger. The steps I took to obtain the correct damping are:
        
        \begin{enumerate}
            \item pinpoint the issue by consulting the TAs about what could've cause the problem;
            \item understand the principles of PID by reading the theories and hypothesize possible combinations that could work
            \item perform initial tuning by controling the variables and setting extreme values, e.g. $D=100$
            \item analyze the plots and generalize the impacts of each parameter in PID
            \item perform fine tuning by selecting better sets of values that maximizes the trend
        \end{enumerate}

    \end{itemize}
    
    \item Mention at least 2 applications where PID controllers would be useful to a robotic implementation (e.g. navigation)
    
    \textbf{Answer: }A proportional–integral–derivative controller is a control loop mechanism employing feedback that is widely used in industrial control systems and a variety of other applications requiring continuously modulated control. The controller continuously calculates an error value $e(t)$ as the difference between a desired setpoint (SP) and a measured process variable (PV) and applies a correction.

    In theory, a PID controller can be used to control any process that has a measurable output (PV), a known ideal value for that output (SP), and an input to the process (MV) that will affect the relevant PV. Controllers are used in industry to regulate temperature, pressure, force, feed rate, flow rate, chemical composition, weight, position, speed, and practically every other variable for which a measurement exists.
    
\end{enumerate}


\end{document}